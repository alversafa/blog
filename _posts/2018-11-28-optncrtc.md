---
layout: post
title: "The Option-Critic Architecture"
excerpt: "This post explains the Option-Critic Architecture. It is largely 
based on the 'The Option-Critic Architecture' paper of P. L. Bacon, J. Harb and 
D. Precup."
date: 2018-11-28 12:35:14 +0300
mathjax: true
---

In reinforcement learning, assigning credits to actions over very large timescales
is very problematic. An agent might take a good action and get its reward very far
in the future. This can cause serious problems in learning good policies. Hierarchical
reinforcement learning is one of the most appealing approaches to this problem as
it facilitates the long timescale credit assignment by dividing the problem into
pieces and then tackling each part.

In this post, I will go over one of the recent (2017) studies on hierarchical 
reinforcement learning from the [Reasoning and Learning Lab](http://rl.cs.mcgill.ca/)
called the Option-Critic Framework which introduces for the first time end-to-end
option (choice, goal) learning. My aim is to make it easier to understand.

For interested readers, an implementation of the algorithm on a jupyter-notebook
can be found on my [github repo](https://github.com/alversafa/option-critic-arch).
(This implementation is based on the [original 
implementation](https://github.com/jeanharb/option_critic/tree/master/fourrooms)
from the authors.)

## 1 Options Framework

The options framework is a hierarchical reinforcement learning framework that was
introduced in [(R. Sutton, D. Precup and S. Singh, 1999)](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf) 
and [(D. Precup, 2000)](www-anw.cs.umass.edu/pubs/2000/precup_thesis00.ps.gz).
Options in this framework stands for higher level policies over lower level policies.
Before moving further, let's first take a look at Fig.1. which is the four-rooms
environment that was proposed in these studies.

<p align="center">
  <img width="30%" src="/assets/images/optncrtc/fourrooms_rand.gif">
</p>
*Fig.1. A randomly acting agent in the four-rooms environment. The initial goal
is the east doorway. The agent starts at a random location and tries to navigate
to the east doorway. After 1000 episodes, the goals moves to a random location in
the lower right room. The actions succeed with probability 2/3 and fail with
probability 1/9 in the other three directions.*

Now, in this environment, if you were placed at a random location and if your
goal was to navigate to the lower right room, how would you achieve the goal?
A natural way would be to come up with some planning like if I am in the 
upper left room, I can navigate to the north doorway and enter the upper right room.
*Then*, I can navigate to the east doorway and reach to my goal. While thinking,
you don't actually think about how to take a step, instead you focus on the big
picture. This is exactly what options in the options framework try to achieve, i.e.
coming up with efficient planning over time. In this framework, every option 
(choice, goal) can be something like exit room 1 (or 2, 3, 4), every door can be a 
termination (switching) point of these options, and every step (primitive action)
the agent takes can be the policies conditioned on the chosen option.

Formally, an option $$\omega$$ has three components:

- $$I_{\omega}$$: initiation set which contains all the states that an option can
start from

- $$\pi_{\omega}(a|s)$$: intra-option policy which is a policy specific to an 
option

- $$\beta_{\omega}(s)$$: termination condition which tells whether an option
terminates or not at a given state

And an option $$\omega$$ is selected according to a policy over options 
$$\pi_{\Omega}(\omega|s)$$ where $$\Omega$$ is the set of possible options.

As in the case of regular reinforcement learning (without hierarchy) where there
are value functions for states ($$V(s)$$) and state-action pairs ($$Q(s,a)$$), 
the options framework also has its own value functions:

- $$Q_{\Omega}(s, \omega)$$: state-option value function

$$
Q_{\Omega}(s, \omega) = \sum_{a} \pi_{\omega}(a|s) Q_{U}(s, \omega, a)
$$

- $$Q_{U}(s, \omega, a)$$: state-option-action value function

$$
Q_{U}(s, \omega, a) = r(s, a) + \gamma \sum_{a} P(s'| s, a) U(s', \omega)
$$

- $$U(s', \omega)$$: utility term

$$
U(s', \omega) = (1 - \beta_{\omega}(s')) Q_{\Omega}(s', \omega) + \beta_{\omega}(s') V_{\Omega}(s')
$$

- $$V_{\Omega}(s')$$: state value function (max of $$Q_{\Omega}(s', \omega)$$ when $$\epsilon$$-greedy
policy is used in option selection)

$$
V_{\Omega}(s') = \max_{\omega} Q_{\Omega}(s', \omega)
$$

- $$A_{\Omega}(s', \omega)$$: advantage function

$$
A_{\Omega}(s', \omega) = Q_{\Omega}(s', \omega) - V_{\Omega}(s')
$$

Now, after defining all of the things related to the options framework, an interesting
question to ask might be: how are these option selected? In the 1999 and 2000 
studies, the authors pre-specify the termination conditions $$\beta_{\omega}(s')$$
to be the hallways. Then, another interesting question pops up: can these options be
learned by the agent on its own? The following section answers this question.


## 2 Option-Critic Framework

After 17 years, the option-critic framework 
[(P. L. Bacon, J. Harb, D. Precup, 2017)](https://arxiv.org/abs/1609.05140) 
extends the  previous options framework by allowing the agent to discover options
*autonomously* while interacting with the environment, i.e. the options are 
*not pre-specified*.

The main contributions of the study are:

- learning options (goals) end-to-end, i.e. without using extra intrinsic rewards, 
demonstrations or any prior knowledge

- a hierarchical reinforcement learning framework which can learn optimal policies
as fast as other methods that have no hierarchy

- a framework which can also work on continuous state and actions spaces

*Note:* As in the case of the previous vanilla options framework, the *number of
options* has to be pre-specified.

The name comes from its similarity to the actor-critic framework. Each of the 
options (which contains a $$\pi_{\omega}(a|s)$$ and a $$\beta_{\omega}(s')$$) are
treated as actors where the critic (which contains the  $$Q_{U}(s, \omega, a)$$ 
and $$A_{\pi_{\Omega}}(s', \omega)$$) sends gradient information to. Fig.2. 
illustrates the similarity between the two frameworks.

<p align="center">
  <img width="100%" src="/assets/images/optncrtc/ac-oc.png">
</p>
*Fig.2. The architecture of the actor-critic and the option-critic frameworks.
Depending on the current option, the intra-option policy and termination conditions
are updated by the critic as in actor-critic. (Image source: 
[Slides](http://pierrelucbacon.com/optioncritic-aaai2017-slides.pdf) of P. L. Bacon)*

In this framework, the policy over options $$\pi_{\Omega}(\omega| s)$$ is learned
by Q-learning with $$\epsilon$$-greedy option selection. (Although it is possible
to use other algorithms.) This policy allows the agent to choose options. After 
the agent chooses an option, it follows its intra-option policy $$\pi_{\omega}(a|s)$$
until the option terminates, then the agent chooses another option and follows its
intra-option policy until termination. This option switching continues until the
agent reaches its ultimate goal. Each of these intra-option policies (which are
Boltzmann distributions) are learned by policy gradients and each of the termination
conditions (which are Bernoulli random variables) are learned by termination gradients.

The derived gradients for the intra-option policy and the termination functions,
parameterized by $$\theta$$ and $$\nu$$ respectively, are:

$$
\frac{\partial Q_{\Omega}(s,\omega)}{\partial \theta} = \mathbb{E} \Bigg[ 
\frac{\partial \pi_{\omega, \theta}(a|s)}{\partial \theta} Q_{U}(s, \omega, a)
\Bigg]
$$

$$
\frac{\partial U(s', \omega)}{\partial \nu} = \mathbb{E} \Bigg[ -
\frac{\partial \beta_{\omega, \nu}(s')}{\partial \nu} A_{\Omega}(s', \omega)
\Bigg]
$$

The gradient for the intra-option policy has the usual interpretation as in the 
case of regular policy gradients: increase the probability of choosing good actions
that lead to rewards. In this framework, this is done for every option independently. 
The gradient for the termination condition on the other hand increases the length
of the options that have good advantages.

The code for the option-critic's main loop is as:

```python
# Initialize all the necessary parameters
for episode in range(nepisodes):
  state = env.reset()
  option = policy_over_options.sample(state)
  action = option_policies[option].sample(state)

  while not done:
    state, reward, done, _ = env.step(action)

    # Check option termination
    if option_terminations[option].sample(state):
    	option = policy_over_options.sample(state)

    action = option_policies[option].sample(state)

    # Critic update
    critic.update_Qs(state, option, action, reward, done, option_terminations)

    # Intra-option policy update
    option_policies[option].update(state, action, Q_U)

    # Termination condition update
    option_terminations[option].update(state, critic.A_Omega(state, option))
```

Below are some of the results I have obtained on the four-rooms environment. First,
let's take a look at Fig.3.:

<p align="center">
  <img width="110%" src="/assets/images/optncrtc/term_prob.png">
</p>
*Fig.3. The termination probabilities of the trained option-critic with 4 options.
The dark blue regions (except for the walls) represent higher termination probability.
In this experiment, the goal was always the east doorway.*

The termination probabilities *learned* by the agent are high near the hallways which 
is intuitively what we would expect. However, since there is no communication between
options, some of them have similar termination probabilities, but it should be
noted that learning good options is still an active research area.

Now, let's take a look at 8 different games played by the trained option-critic
agent in Fig.4.:


<table width="100" border="0">
<tr>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep0.gif">
</td>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep1.gif">
</td>

</tr>
</table>

<table width="500" border="0">
<tr>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep2.gif">
</td>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep3.gif">
</td>

</tr>
</table>

<table width="500" border="0">
<tr>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep4.gif">
</td>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep5.gif">
</td>

</tr>
</table>

<table width="500" border="0">
<tr>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep6.gif">
</td>

<td align="center" valign="center">
<img width="100%" src="/assets/images/optncrtc/test_ep7.gif">
</td>

</tr>
</table>


*Fig.4. 8 different games played by the trained option-critic agent in slow motion.
Each gif displays the agent's position on the left and it currently active option
together with the option's termination map on the right. In these experiments, the
goal is always the east doorway and it is colored in white like the agent. The 
probability of an action failing is again 1/3. It should be noted that the sequence
of options and their meanings can change from run to run. The results above are
for just one of the runs.*

Below are some interesting observations on the agents behavior:

- Episode 0, 3: Since the agent is initialized near the goal, it just chooses a
single option and follows its intra-option policy without any problem.

- Episode 1: The agent first chooses option 0 to get close to the goal and then
switches to option 3 to reach it.

- Episode 2: The agent starts 2 steps away from the goal and chooses option 3 to 
reach it; however, due to the stochasticity of the environment and its own policy,
it ends up in unpleasant situations.

- <ins>Episode 4, 7:</ins> These are interesting ones because the agent first starts
in the upper left room, chooses option 3 to reach the north doorway, then chooses
option 0 to get close to the goal, and then finally chooses option 3 again to reach
it.

- Episode 5: The agent chooses option 1 and then follows with option 0 and option 3.
But it gets very unlucky on the way and wonders around for 27 steps.

- Episode 6: The agent starts with option 2, then chooses option 0 to get close to
the goal and finally choose option 3 to reach it; however, its action fails and
its arrival gets a bit delayed.

Although learning these options adds an additional computational cost, it is shown
that learning options allows the agent to adapt to new situations faster, i.e. it
allows transfer learning. (See
Fig.5.)

<p align="center">
  <img width="80%" src="/assets/images/optncrtc/recover.png">
</p>
*Fig.5. An experiment done by the authors. The first goal is the east doorway. After
1000 episodes, the goal moves to a random loaction in the lower right room. The
option-critic agent reaches the performance of SARSA(0) and AC-PG in the first 
task. Moreover, it recovers much faster when the goal moves.
(Image source: [Slides](http://pierrelucbacon.com/optioncritic-aaai2017-slides.pdf)
of P. L. Bacon)*

The authors also tested their algorithm on the ATARI games and achieved comparable or 
better performances than [DQN](https://arxiv.org/abs/1312.5602). In these ATARI experiments,
the option-critic found interpretable options in the game 
[Sequest](https://gym.openai.com/envs/Seaquest-v0/) when 2 options were used. One of the
options turned out to be the the places near the surface of the ocean and the other one
turned out to be the places near the bottom of it. (The transitions between options happen
while going from top to bottom and vice-versa.) Check out their [paper](https://arxiv.org/abs/1609.05140)
for more details on the experiments and the tricks they used.

## 3 Other Hierarchical Reinforcement Learning Methods

Another very interesting study on hierarchical reinforcement learning which also is end-to-end is 
[FeUdal Networks](https://arxiv.org/abs/1703.01161) from DeepMind. I'll try to explain
this paper in  a future post.

&nbsp;

---  

&nbsp;

*If you notice any mistakes and errors in this post or want to make it better, 
please contact me through [alver dot safa at gmail dot com]. I would be happy to
correct / improve it :)*

&nbsp;

## References

[1] P. L. Bacon, J. Harb, D. Precup. 
["The Option-Critic Architecture"](https://arxiv.org/abs/1609.05140). AAAI 2017.

[2] [Slides](http://pierrelucbacon.com/optioncritic-aaai2017-slides.pdf) of P. L.
Bacon at AAAI 2017.

[3] R. Sutton, D. Precup, S. Singh.
["Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement
Learning"](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf).
Artificial Intelligence 1999.

[4] D. Precup. 
["Temporal Abstraction in Reinforcement Learning"](www-anw.cs.umass.edu/pubs/2000/precup_thesis00.ps.gz).
PhD Dissertation 2000.
