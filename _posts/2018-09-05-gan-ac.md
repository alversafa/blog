---
layout: post
title: "Connections Between GANs and AC Methods in Reinforcement Learning"
excerpt: "This post illustrates the connection between Generative Adversarial
Networks and Actor-Critic methods in Reinforcement Learning. It is largely 
based on the 'Connecting Generative Adversarial Networks and Actor-Critic Methods'
paper of D. Pfau & O. Vinyals."
date: 2018-09-04 18:32:14 +0300
mathjax: true
---

When I first read the Generative Adversarial Nets (GAN) [paper](https://arxiv.org/abs/1406.2661),
I was already familiar with reinforcement learning and had the feeling that these
two are somehow connected. After doing some research, I have come across the 2017
[study](https://arxiv.org/abs/1610.01945) of [David Pfau](http://davidpfau.com/) and
[Oriol Vinyals](http://www1.icsi.berkeley.edu/~vinyals/) from [DeepMind](https://deepmind.com/).
In this post, I will go over their study and try to explain the connection between
GANs and reinforcement learning (Actor-Critic methods). As in the 
[paper](https://arxiv.org/abs/1610.01945), I will first start by reviewing the two
methods and then move on to the connection between the two. My aim is to make the
connection easy to understand.

Readers who are already familiar with GANs and AC methods (DPG, SVG, NFQCA) can 
quickly skim over Sections 1 & 2 to just get familiar with the notation and then
continue reading the rest of the post.

## 1 Generative Adversarial Nets

[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661) (GAN) are generative
models that were introduced by I. Goodfellow et. al in 2014. They work according to
a [minimax game](https://en.wikipedia.org/wiki/Minimax) between two neural networks
called the generator $$G$$ and the discriminator $$D$$ which
have the following input-output relationships:

- $$G$$ takes a latent $$z$$ as input and outputs a synthetic data $$X_{gen}$$.
  The input $$z$$ is usually sampled from zero-mean identity-covariance multivariate 
  Gaussian $$\mathcal{N}(0, I)$$ and the output $$X_{gen}$$ is usually an image.

- $$D$$ takes the real data $$X_{real}$$ and the synthetic data $$X_{gen}$$ as
  input and outputs the probability of the input being real. So the output is 
  basically a single scalar between $$0$$ and $$1$$.

At each training cycle, the generator $$G$$ tries to get better at fooling the 
discriminator $$D$$ while the discriminator $$D$$ tries to not get fooled. The 
typical high-level architecture of GANs is as in Fig. 1.


<p align="center">
  <img width="80%" src="{{ site.baseurl }}/assets/images/gan-ac/gan.png">
</p>
*Fig. 1. Architecture of generative adversarial networks. Solid lines represent
the data flows, dotted lines represent gradient flows. The discriminator uses
the data generated by the generator. Then a loss is computed and the gradients
are passed back to the parameters of the networks. This continues until order 
emerges from chaos in which the generator learns to produce reallistically looking
data and the discriminator learns to become a good judge.*

More formally, a minimax game with the following value function $$V(D, G)$$ is 
being played between $$G$$ and $$D$$:

$$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{X \sim p_{real}(X)} [\log D(X)] +
\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]
$$

So in each training iteration:

- $$D$$ tries to maximize the following loss $$L_{D}(D, G)$$:

$$
L_{D}(D, G) = \mathbb{E}_{X \sim p_{real}(X)} [\log D(X)] +
\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]
$$

- $$G$$ tries to minimize the following loss $$L_{G}(D, G)$$:

$$
L_{G}(D, G) = \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]
$$

*Note:* The first term is thrown away in $$L_{G}(D, G)$$ since it is just a constant
for $$G$$.

In the [GAN paper](https://arxiv.org/abs/1406.2661), there is a suggested training
trick to maximize the expectation of $$\log D(G(z))$$ rather than minimizing the 
expectation of $$\log(1 - D(G(z)))$$. The idea behind the trick is to prevent 
*saturated* gradients in the early phases of training which is caused by $$D$$ 
becoming smarter quickly than expected. (If this isn't obvious, just imagine 
$$D(G(z)) \rightarrow 0$$ and look at $$\nabla_{\theta_G} L_{G}(D, G) $$ when 
$$L_{G}(D, G)$$ is two of the losses discussed. You will observe that the suggested
tricky loss has the potential to punish $$G$$ more when $$D$$ gets smarter.) It 
should also be noted that a maximization problem can be turned into a minimization
one by just placing a minus sign in front of the loss function and vice-versa.

So, with the tricks given above, the problem can be turned into a bilevel minimization
problem, i.e. in each training iteration:

- $$D$$ tries to minimize the following loss $$L_{D}(D, G)$$:

$$
L_{D}(D, G) = -\mathbb{E}_{X \sim p_{real}(X)} [\log D(X)] -
\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]
$$

- $$G$$ tries to minimize the following loss $$L_{G}(D, G)$$:

$$
L_{G}(D, G) = -\mathbb{E}_{z \sim p_{z}(z)} [\log D(G(z))]
$$

At *optimal* convergence, $$G$$ is expected to produce realistically looking data
and $$D$$ is expected to not being able to tell the difference between generated
and real data, i.e. it should output $$0.5$$.

## 2 Actor-Critic Methods

[Actor-Critic methods](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)
(AC) are methods in reinforcement learning which are *combinations* of value-based
learning methods (as value iteration, MC methods and TD-learning) and policy-based
learning methods (as policy gradients). There are several types of AC methods, however
we will focus only on the ones in which the critic outputs an action-value function
$$Q(s,a)$$ and the actor outputs a policy $$\pi$$. (The reason will become clear in
Section 3.) The methods considered in the study of Pfau & Vinyals are [Deterministic
Policy Gradients (DPG)](http://proceedings.mlr.press/v32/silver14.pdf) by D. Silver 
et. al, [Stochastic Value Gradients (SVG)](https://arxiv.org/abs/1510.09142) by N. 
Heess et. al and [Neurally-Fitted Q-Learning with Continous Actions (NFQCA)](https://link.springer.com/article/10.1007/s10994-011-5235-x) 
by R. Hafner et. al. Since it doesn't matter which one we choose to relate to GANs,
I'll continue the discussion with DPG and use it interchangeably with AC methods.

*Note:* One of the most popular AC method is the
[Asynchronous Advantage Actor-Critic (A3C)](https://arxiv.org/abs/1602.01783) proposed
by V. Mnih et. al in 2016. This study has shown a great success in both discrete and
continuous action spaces, however the critic in A3C outputs the state value function
$$V(s)$$. Since $$V(s)$$ contains no information about the action it cannot provide
gradients to the actor for its actions. This makes the A3C method unsuitable for the
setting which will be described in Section 3.

In DPG type AC methods there are again two neural nets which are called the actor
$$A$$ and the critic $$C$$. This time there is *no* minimax game and they have the 
following input-output relationships:

- $$A$$ takes the state $$s_t$$ as input and outputs a policy $$\pi$$. The policy
  can be either deterministic (DPG) or stochastic (SVG).

- $$C$$ takes the state $$s_{t}$$ as input and outputs the action-value function 
  $$Q(s_{t}, a_{t})$$ in which the action $$a_t$$ is choosen according to the policy
  $$\pi$$ from $$A$$.

At each training cycle, the actor $$A$$ gets better at predicting the optimal policy
and the critic $$C$$ gets better at approximating the state-action value function
$$Q(s, a)$$. The typical high-level architecture of DPG type AC methods is as in
Fig. 2.

<p align="center">
  <img width="75%" src="{{ site.baseurl }}/assets/images/gan-ac/ac.png">
</p>
*Fig. 2. Architecture of the deterministic policy gradients type methods. Solid lines
represent data flow, dotted lines represent gradient flow. The critic uses the data 
generated by the actor. Then gradients are computed and passed back to the parameters
of the networks. This again continues until order emerges from chaos in which actor 
learns good policies and the critic learns to be a good predictor of future rewards.*

To be more formal, consider a Markov Decision Process (MDP) with states $$\mathcal{S}$$,
actions $$\mathcal{A}$$, a distribution over initial states $$p_{0}(s)$$, transition
function $$P(s_{t+1}|s_{t}, a_{t})$$, reward distribution $$R(s_{t})$$ and discount
factor $$\gamma$$. In this setting, the action-value function $$Q(s, a)$$ given a policy
$$\pi$$ is defined as follows:

$$
\begin{aligned}
Q^{\pi}(s, a) &= \mathbb{E}_{s_{t+k} \sim P, r_{t+k} \sim R, a_{t+k} \sim \pi} \bigg[ 
\gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + ... 
\bigg| s_{t} = s, a_{t} = a \bigg] \\

&= \mathbb{E}_{s_{t+k} \sim P, r_{t+k} \sim R, a_{t+k} \sim \pi} \bigg[ 
\sum_{k=1}^{\infty} \gamma^{k} r_{t+k} \bigg| s_{t} = s, a_{t} = a \bigg]
\end{aligned}
$$

And the *optimal* policy $$\pi^{\ast}$$ can be obtained from $$Q^{\pi}(s,a)$$ as:

$$
\pi^{\ast} = \arg \max_{\pi} \mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi}[Q^{\pi}(s_{0}, a_{0})]
$$

The training procedure of DPG type methods can also be expressed as a bilevel
minimization problem in which every training step:

- $$C$$ tries to minimize the following loss $$L_{C}(Q, \pi)$$:

$$
L_{C}(Q, \pi) = \mathbb{E}_{s_{t}, a_{t} \sim \pi}[\mathcal{D} (\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}} [r_{t} + \gamma Q(s_{t+1}, a_{t+1})] || Q(s_{t}, a_{t}))]
$$

- $$A$$ tries to minimize the following loss $$L_{A}(Q, \pi)$$:

$$
L_{A}(Q, \pi) = -\mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi} [Q(s_{0}, a_{0})]
$$

*Note:* $$Q(.) \equiv C(.)$$ and $$\pi (.) \equiv A(.)$$. $$\mathcal{D}(.||.)$$ is
any non-negative divergence.

At *optimal* convergence, $$A$$ is expected to produce policies that maximize the
total discounted future reward and $$C$$ is expected to become a good predictor
of the future.

## 3 Connection

Now, first of all, notice that in both GANs and AC methods only
one neural net ($$D$$ in GANs and $$C$$ in AC) is aware of the reward 
signals from the environment. The other neural nets ($$G$$ in GANs and $$A$$ in
AC) only get this information through the gradients of the first networks. The
high-level information flow structure in these two methods is as in Fig. 3.


<p align="center">
  <img width="75%" src="{{ site.baseurl }}/assets/images/gan-ac/gan-ac.png">
</p>
*Fig. 3. Information flow in GANs (a) and AC methods (b). Again, solid lines represent
data flow, dotted lines represent gradient flow. Analogous flows are highlighted
in dark red. In both methods, one network has access to the reward information coming
from the environment ($$r_t$$ for $$C$$ and $$y$$ (label for fake or real) for $$D$$) and
the other network uses the gradient coming from the first one to get informed.
(There are also gradient flows from the networks to themselves, but they are not
shown for simplicity.) (Image source: Reproduced from the 
[paper](https://arxiv.org/abs/1610.01945) of Pfau & Vinyals)*

The connection can be illustrated further by moving the GAN game (the minimax game)
to an RL setting and treating $$D$$ as a $$C$$ and $$G$$ as an $$A$$ in AC methods.
In their [study](https://arxiv.org/abs/1610.01945), Pfau & Vinyals  define an MDP
in which:

- *action* is an adjustment of every pixel in a blank image ($$A$$ is the one who takes
  actions)

- *state* is either the generated image (by $$A$$) or the real image (the environment
   randomly decides which one to show)

- *reward* is $$1$$ if the real image is choosen by the environment and $$0$$ otherwise
  (notice that rewards are independent of $$A$$'s actions)

*Note:* The MDP is stateless (we have one state, take an action, get a reward and
the game ends) since the action taken by $$A$$ does not have an effect on future states.

Now, we have an AC style learning in an RL setting resembles the GAN game. However,
there are still some modifications needed to make it identical:

- $$A$$ cannot have access to the state, i.e. $$A$$ is *blind*. If it had, it would just
  learn to *copy* the real images at every step to minimize 
  $$-\mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi} [Q^{\pi}(s_{0}, a_{0})]$$.
  (Note that in the original training of GANs, we also do not show $$G$$ the
  real images. In Fig. 3a, there is no arrow from $$x$$ to $$G$$.) $$A$$ 
  generates its actions from a random latent space.

- $$A$$ receives a gradient of $$0$$ if a real image is selected by the environment.
  (Again note that in the original training of GANs, $$G$$'s loss doesn't have a term
  that depends on the real image.)

The interaction between the agent and the environment in an example GAN game is as in 
Fig. 4.

<p align="center">
  <img width="80%" src="{{ site.baseurl }}/assets/images/gan-ac/game.png">
</p>
*Fig. 4. An example GAN game formulated in an RL setting. Images are binary images and
an action corresponds to setting a pixel either to 1 or 0. The environment randomly
chooses to show either the real image or the generated one. The agent receives a reward
of 1 if the environment chooses a real image and recieves a reward of 0 if it doesn't.
The actions taken by the agent have no effect on the reward.*

&nbsp;

> "GANs can be seen as a modified AC method with blind actors in stateless MDPs." 
  -- Pfau & Vinyals

Since training in both GANs and AC methods is unstable, the authors also discuss
several stabilizing strategies used to improve training in both methods in their
[paper](https://arxiv.org/abs/1610.01945). However, in this post
I will not go over them and I suggest interested readers to go over their paper.


## 4 Adversarial Behavior in AC

In AC methods, $$A$$ and $$C$$ typically optimize a loss function in a complementary
fashion. An interested reader might ask: "How does the adversarial behavior arise?"
To answer this question let's think about the training process of the modified
AC method. The loss of the critic, defined in Section 2, was:

$$
L_{C}(Q, \pi) = \mathbb{E}_{s_{t}, a_{t} \sim \pi} [\mathcal{D} (\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}} [r_{t} + \gamma Q(s_{t+1}, a_{t+1})] || Q(s_{t}, a_{t}))]
$$

Since we have a stateless MDP (there is no next state, the game ends after taking
a single action and receiving a reward) the loss of the critic takes the following form:

$$
L_{C}(Q, \pi) = \mathbb{E}_{s_{0} \sim p_{0}, r_{0} a_{0} \sim \pi} [\mathcal{D} (r_{0} || Q(s_{0}, a_{0}))]
$$

The loss above provides gradients for the critic to change its parameters to output
a $$Q(s_{0}, a_{0})$$ close to $$1$$ when $$s_0$$ is a real image ($$I_{real}$$) and 
output a $$Q(s_{0}, a_{0})$$ close to $$0$$ when $$s_0$$ is a generated image ($$I_{gen}$$).
Since the rewards are independent of the actions, $$C$$ learns to not pay attention
to the actions and focuses on the states.

Now let's move on to to the actor. From $$A$$'s perspective the situation is a 
little different. $$A$$ thinks that its actions can make a difference. Remember
that $$A$$ had the following loss:

$$
L_{A}(Q, \pi) = -\mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi} [Q(s_{0}, a_{0})]
$$

When $$A$$ takes an action, it basically adjusts the pixels of a blank image it has
at the beginning. When it selects actions that lead to images that resemble real
images, it results in a high $$Q(s_{0}, a_{0})$$ (assuming $$C$$ is trained a bit)
and $$A$$ recieves gradients that encourage these actions which in turn will result
in higher $$Q(s_{0}, a_{0})$$'s in each training step. But, remember that $$C$$ is
being trained in a way to lower the $$Q(s_{0}, a_{0})$$'s when $$s_{0} = I_{gen}$$.
So in each training step $$C$$ tries to decrease $$Q(I_{gen}, a_{0})$$ while $$A$$
tries to increase it. This is where the *adversarial behavior* arises. 

> $$A$$ lives in a world where it thinks that its actions can make a difference while
  $$C$$ doesn't care about them at all. During the training process, $$A$$ becomes a
  master gradually and $$C$$ starts to treat the outcomes of $$A$$'s work seriously.


## 5 Other Connections in RL

GANs also have connections to other methods in RL as Imitation Learning and Inverse
Reinforcement Learning. (The details can be found in the [paper](https://arxiv.org/abs/1610.01945).)
I'll try to explain these connections in a future post.  

&nbsp;

---  

&nbsp;

*If you notice any mistakes and errors in this post or want to make it better, 
please contact me through [alver dot safa at gmail dot com]. I would be happy to
correct / improve it :)*
